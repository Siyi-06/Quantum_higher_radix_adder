# Readme

This is the code for our paper "A Higher Radix Architecture for Quantum Carry-lookahead Adder.([https://arxiv.org/abs/2304.02921](https://arxiv.org/abs/2304.0292)) The code is based on the **Quantify** framework (link: [https://github.com/quantumresource/quantify](https://github.com/quantumresource/quantify)).

In this repository, we have accomplished the following:

1. Implemented a quantum Higher radix adder in "AdderExample": The code includes the implementation of the proposed higher radix quantum adder using the **Quantify** framework. This implementation improves upon existing adder designs by leveraging a higher radix architecture, resulting in improved efficiency.

2. Developed comparison programs in "Adders_Comparison": We have included a comparison program that allows readers to evaluate and compare the performance of different adder designs.

3. Established a quantum adder library: We have established an adder library, which systematically implements various quantum adders within the same program framework.
   
4. Fixed small programming glitches in the construction of Draper adder: We have identified and corrected small issues in the construction of the Draper adder. These corrections enhance the accuracy and functionality of the adder.


## Usage

To use the code in this repository, please follow the instructions below:

1. Clone the **Quantify** framework repository from [https://github.com/quantumresource/quantify](https://github.com/quantumresource/quantify) and follow the installation instructions provided in the repository.

2. Once the **Quantify** framework is set up, clone or download the code from this repository, and proceed to execute it.

## Acknowledgments

We would like to express our gratitude to the contributors and maintainers of the **quantify** framework for their valuable work. 

## Contact Information

If you have any questions or need further assistance regarding the code in this repository, please feel free to contact:

SIYI002@e.ntu.edu.sg

Thank you for your interest in our research.

Sincerely,
Siyi
